{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Titanic using  Classification Trees.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "laXn8MNnFPky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13ad8ad4-6c37-4573-8d72-aac4ece91844"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DATT-2ls5sYc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre Processing:**"
      ],
      "metadata": {
        "id": "FYSvYETi8CD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Droping cabin since 77% is NaN, and Ticket and Name are not informative.\n",
        "## Turning Sex and Embarked into One Hot encodings\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def pre_proccess(up_set, isTrain=True):\n",
        "  avg_age = np.floor(up_set['Age'][up_set['Age'].isna()==False].mean())\n",
        "  up_set['Age'][up_set['Age'].isna()] = avg_age\n",
        "\n",
        "  up_set_proc = up_set.drop(columns=['Ticket', 'Cabin', 'Name','Embarked'])\n",
        "  up_set_proc['Fare'][up_set_proc['Fare'].isna()] = 0\n",
        "  # if isTrain:\n",
        "  #   drop_idx = up_set_proc[up_set_proc['Embarked'].isna()].index\n",
        "  #   up_set_proc = up_set_proc.drop(index=drop_idx)\n",
        "  up_set_proc['Sex'][up_set_proc['Sex']=='male'] = 0\n",
        "  up_set_proc['Sex'][up_set_proc['Sex']=='female'] = 1\n",
        "  up_set_proc['Sex'] = up_set_proc['Sex'].astype(int)\n",
        "\n",
        "  # enc_features = OneHotEncoder().fit_transform(up_set_proc[['Embarked']]).toarray()\n",
        "  # enc_features = np.array(list(zip(enc_features)))\n",
        "  # up_set_proc = up_set_proc.drop(columns='Embarked')\n",
        "  # up_set_proc['Embarked'] = [x[0][0] for x in list(zip(enc_features))]\n",
        "  return up_set_proc"
      ],
      "metadata": {
        "id": "hSI4o2ty8A6s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Droping cabin since 77% is NaN, and Ticket and Name are not informative.\n",
        "\n",
        "def pre_proccess_2(up_set, isTrain=True):\n",
        "  # Setting all NaN ages to the average age. The same with Fare.\n",
        "  avg_age = np.floor(up_set['Age'][up_set['Age'].isna()==False].mean())\n",
        "  up_set['Age'][up_set['Age'].isna()] = avg_age\n",
        "  avg_fare = np.floor(up_set['Fare'][up_set['Fare'].isna()==False].mean())\n",
        "  up_set['Fare'][up_set['Fare'].isna()] = avg_fare\n",
        "\n",
        "  # Droping irrelevant columns, and changing catagorical columns to numeric vals.\n",
        "  up_set_proc = up_set.drop(columns=['Ticket','Embarked', 'Name', 'Cabin'])\n",
        "  up_set_proc['Sex'][up_set_proc['Sex']=='male'] = 0\n",
        "  up_set_proc['Sex'][up_set_proc['Sex']=='female'] = 1\n",
        "  up_set_proc['Sex'] = up_set_proc['Sex'].astype(int)\n",
        "\n",
        "\n",
        "  # Max Abs normalization:\n",
        "  for col in ['Age', 'Fare', 'SibSp', 'Parch']:\n",
        "    up_set_proc[col] = up_set_proc[col] /up_set_proc[col].abs().max()\n",
        "\n",
        "\n",
        "  return up_set_proc"
      ],
      "metadata": {
        "id": "QiSOs10adY2v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepering Train/Val/Test sets:**"
      ],
      "metadata": {
        "id": "cf1WZkMOi0D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_up = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Kaggle/Titanic/train.csv', index_col='PassengerId')\n",
        "test_set_up = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Kaggle/Titanic/test.csv', index_col='PassengerId')\n",
        "train_set_p = pre_proccess_2(train_set_up)\n",
        "test_set_p = pre_proccess_2(test_set_up, isTrain=False)\n"
      ],
      "metadata": {
        "id": "nHcZRmCsTG8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a21a9f1-33ad-47b3-da58-bfff91808e29"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training:**"
      ],
      "metadata": {
        "id": "P1kwD0O7Wnkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting Tree:**"
      ],
      "metadata": {
        "id": "wbVedpZAo5Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_set, val_set = train_test_split(train_set_p,test_size=300, random_state=42)\n",
        "\n",
        "train_set_y = train_set['Survived']\n",
        "train_set_x = train_set.drop(columns='Survived')\n",
        "val_set_y = val_set['Survived']\n",
        "val_set_x = val_set.drop(columns='Survived')\n",
        "loss_list = []\n",
        "for est in range(50,800,50):\n",
        "  for lr in [0.1,0.01,0.001,0.0001]:\n",
        "    for md in [3,4,5,7,10.15,20,25,30]:\n",
        "      model_gb = GradientBoostingClassifier(max_depth=md,n_estimators=est, learning_rate=lr)\n",
        "      model_gb.fit(train_set_x, train_set_y)\n",
        "      loss_list.append((est,lr,md,model_gb.score(val_set_x, val_set_y)))"
      ],
      "metadata": {
        "id": "g7A60vsIWqXB"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list.sort(key= lambda x: x[3], reverse=True)\n",
        "loss_list[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llaPMPb4jb1s",
        "outputId": "5fdb14b8-1809-4f96-d499-dc605a3362b1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(550, 0.01, 5, 0.8433333333333334),\n",
              " (600, 0.01, 5, 0.8433333333333334),\n",
              " (50, 0.1, 5, 0.84),\n",
              " (250, 0.01, 3, 0.84),\n",
              " (300, 0.01, 3, 0.84),\n",
              " (350, 0.01, 3, 0.84),\n",
              " (200, 0.01, 3, 0.8366666666666667),\n",
              " (650, 0.01, 5, 0.8366666666666667),\n",
              " (700, 0.01, 5, 0.8366666666666667),\n",
              " (200, 0.1, 5, 0.8333333333333334)]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_y = train_set['Survived']\n",
        "train_set_x = train_set.drop(columns='Survived')\n",
        "val_set_y = val_set['Survived']\n",
        "val_set_x = val_set.drop(columns='Survived')\n",
        "md = 5\n",
        "est = 600\n",
        "lr = 0.01\n",
        "\n",
        "model = GradientBoostingClassifier(max_depth=md,n_estimators=est, learning_rate=lr)\n",
        "model.fit(train_set_x, train_set_y)\n",
        "y_pred = model.predict(test_set_p)\n",
        "test_set_p['Survived'] = y_pred\n",
        "final_df = test_set_p['Survived']\n",
        "final_df.to_csv(f'prediction_{md}_{est}_{lr}.csv')"
      ],
      "metadata": {
        "id": "trBbPOtmxHiC"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBooster:**"
      ],
      "metadata": {
        "id": "vn_sQ9Wbo_SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "train_set_y = train_set['Survived']\n",
        "train_set_x = train_set.drop(columns='Survived')\n",
        "val_set_y = val_set['Survived']\n",
        "val_set_x = val_set.drop(columns='Survived')\n",
        "for est in range(50,800,50):\n",
        "  for lr in [0.1,0.01,0.001]:\n",
        "    for md in [3,4,5,7,10,15,20,25,30]:\n",
        "      model_xgd =xgb.XGBClassifier(max_depth=md,n_estimators=est, learning_rate=lr)\n",
        "      model_xgd.fit(train_set_x, train_set_y)\n",
        "      loss_list.append((est,lr, md,model_xgd.score(val_set_x, val_set_y)))"
      ],
      "metadata": {
        "id": "WvcrAe6ichc3"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list.sort(key= lambda x: x[3], reverse=True)\n",
        "loss_list[:10]"
      ],
      "metadata": {
        "id": "ayGKxb3rmfhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forrest:**"
      ],
      "metadata": {
        "id": "fRjOVf1CpBf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
        "\n",
        "train_set, val_set = train_test_split(train_set_p,test_size=300,random_state=10)\n",
        "train_set_y = train_set['Survived']\n",
        "train_set_x = train_set.drop(columns='Survived')\n",
        "val_set_y = val_set['Survived']\n",
        "val_set_x = val_set.drop(columns='Survived')\n",
        "\n",
        "loss_list = []\n",
        "for est in range(50,800,50):\n",
        "    for md in [3,4,5,7,10,15,20,25,30]:\n",
        "      for bs in [True, False]:\n",
        "        model_rf =RandomForestClassifier(max_depth=md, n_estimators=est, bootstrap=bs,random_state=10)\n",
        "        model_rf.fit(train_set_x, train_set_y)\n",
        "        loss_list.append((est, md, bs,model_rf.score(val_set_x, val_set_y)))"
      ],
      "metadata": {
        "id": "5Sz8YhhQo1Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list.sort(key= lambda x: x[3], reverse=True)\n",
        "loss_list[:10]"
      ],
      "metadata": {
        "id": "jOpCmsKTo4MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict:**"
      ],
      "metadata": {
        "id": "gq2tEQL0g7K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_y = train_set['Survived']\n",
        "train_set_x = train_set.drop(columns='Survived')\n",
        "val_set_y = val_set['Survived']\n",
        "val_set_x = val_set.drop(columns='Survived')\n",
        "\n",
        "model = RandomForestClassifier(max_depth=5, n_estimators=50, bootstrap=False,random_state=10)\n",
        "model.fit(train_set_x, train_set_y)\n",
        "y_pred = model.predict(test_set_p)\n",
        "test_set_p['Survived'] = y_pred\n",
        "final_df = test_set_p['Survived']\n",
        "final_df.to_csv('prediction.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIedUsoKkM0p",
        "outputId": "5ce783b6-3a37-4c35-e5e6-555eb3a7f4ef"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8433333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    }
  ]
}