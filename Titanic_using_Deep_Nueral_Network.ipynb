{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Titanic using Deep Nueral Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNG6TLsZE7yb",
        "outputId": "3efcc30b-18c7-4454-9617-9e357ba0535d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DATT-2ls5sYc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# importing train and test sets from google drive\n",
        "train_set_up = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Kaggle/Titanic/train.csv', index_col='PassengerId')\n",
        "test_set_up = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Kaggle/Titanic/test.csv', index_col='PassengerId')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre Processing:**"
      ],
      "metadata": {
        "id": "FYSvYETi8CD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Droping cabin since 77% is NaN, and Ticket and Name are not informative.\n",
        "\n",
        "def pre_proccess(up_set, isTrain=True):\n",
        "  # Setting all NaN ages to the average age. The same with Fare.\n",
        "  avg_age = np.floor(up_set['Age'][up_set['Age'].isna()==False].mean())\n",
        "  up_set['Age'][up_set['Age'].isna()] = avg_age\n",
        "  avg_fare = np.floor(up_set['Fare'][up_set['Fare'].isna()==False].mean())\n",
        "  up_set['Fare'][up_set['Fare'].isna()] = avg_fare\n",
        "\n",
        "  # Droping irrelevant columns, and changing catagorical columns to numeric vals.\n",
        "  up_set_proc = up_set.drop(columns=['Ticket','Embarked', 'Name', 'Cabin'])\n",
        "  up_set_proc['Sex'][up_set_proc['Sex']=='male'] = 0\n",
        "  up_set_proc['Sex'][up_set_proc['Sex']=='female'] = 1\n",
        "  up_set_proc['Sex'] = up_set_proc['Sex'].astype(int)\n",
        "\n",
        "\n",
        "  # Max Abs normalization:\n",
        "  for col in ['Age', 'Fare', 'SibSp', 'Parch']:\n",
        "    up_set_proc[col] = up_set_proc[col] /up_set_proc[col].abs().max()\n",
        "\n",
        "\n",
        "  return up_set_proc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self,df, isTest=False):\n",
        "    self.isTest = isTest\n",
        "    self.indexes = df.index.to_numpy()\n",
        "    if self.isTest==False:\n",
        "      y=df['Survived'].to_numpy()\n",
        "      self.y_train=torch.tensor(y,dtype=torch.float32,requires_grad=True)\n",
        "      x=df.drop(columns='Survived').to_numpy()\n",
        "      self.x_train=torch.tensor(x,dtype=torch.float32,requires_grad=True)\n",
        "    else:\n",
        "      x=df.to_numpy()\n",
        "      self.x_train=torch.tensor(x,dtype=torch.float32, requires_grad=True)\n",
        " \n",
        "  def __len__(self):\n",
        "    return self.x_train.shape[0]\n",
        "   \n",
        "  def __getitem__(self,idx):\n",
        "    if self.isTest==False:\n",
        "      return self.indexes[idx],self.x_train[idx],self.y_train[idx]\n",
        "    else:\n",
        "      return self.indexes[idx],self.x_train[idx]\n",
        "\n",
        "\n",
        "def errors_per_batch(outputs, labels):\n",
        "  error = 0\n",
        "  for i in range(len(labels)):\n",
        "    if outputs[i] != labels[i]:\n",
        "      error += 1\n",
        "  return error"
      ],
      "metadata": {
        "id": "hSI4o2ty8A6s"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Creation:**"
      ],
      "metadata": {
        "id": "fueE7hR8L4F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, dout1 = 0.5,dout2 = 0.5,dout3 = 0.5):\n",
        "      super(Net, self).__init__()\n",
        "      self.layer1 = nn.Sequential(\n",
        "          nn.Linear(6, 10),\n",
        "          nn.Dropout1d(dout1),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "\n",
        "      self.layer2 = nn.Sequential(\n",
        "          nn.Linear(10, 20),\n",
        "          nn.Dropout1d(dout1),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "\n",
        "      self.layer3 = nn.Sequential(\n",
        "        nn.Linear(20, 40),\n",
        "        nn.Dropout1d(dout2),\n",
        "        nn.ReLU()\n",
        "      )\n",
        "\n",
        "      self.layer4 = nn.Sequential(\n",
        "        nn.Linear(40, 2),\n",
        "        nn.Dropout1d(dout3),\n",
        "        nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "      self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = self.layer1(x)\n",
        "      out = self.layer2(out)\n",
        "      out = self.layer3(out)\n",
        "      out = self.layer4(out)\n",
        "      return self.softmax(out)\n"
      ],
      "metadata": {
        "id": "As4gfIvaL52v"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training**"
      ],
      "metadata": {
        "id": "EODPSLNPYdBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(d_out, learning_rate, epochs ,train_loader, val_loader):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  net_model = Net(d_out, d_out ,d_out)\n",
        "  net_model = net_model.cuda(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(net_model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for i, (_,points, labels) in enumerate(train_loader):\n",
        "      if torch.cuda.is_available():\n",
        "        points = points.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "      outputs = torch.argmax(net_model(points), dim=1)\n",
        "      outputs = outputs.type(torch.float32)\n",
        "      labels.retain_grad()\n",
        "      loss = criterion(outputs, labels)\n",
        "      # optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  net_model.eval()\n",
        "  t_loss = 0\n",
        "  total_loss = [t_loss]\n",
        "  loss_per_batch = []\n",
        "  val_one_loss_total = []\n",
        "  train_one_loss_total = []\n",
        "  with torch.no_grad():\n",
        "    for i, (_,points, labels) in enumerate(val_loader):\n",
        "      if torch.cuda.is_available():\n",
        "        points = points.cuda()\n",
        "        labels = labels.cuda()\n",
        "      outputs = torch.argmax(net_model(points), dim=1)\n",
        "      outputs = outputs.type(torch.float32)\n",
        "      loss = criterion(outputs, labels)\n",
        "      error_loss = errors_per_batch(outputs, labels)\n",
        "      val_one_loss_total.append(error_loss)\n",
        "      loss = loss.data.cpu().detach().numpy()\n",
        "      t_loss += loss\n",
        "      total_loss.append(t_loss)\n",
        "      loss_per_batch.append(loss)\n",
        "\n",
        "    for i, (_,points, labels) in enumerate(train_loader):\n",
        "      if torch.cuda.is_available():\n",
        "        points = points.cuda()\n",
        "        labels = labels.cuda()\n",
        "      outputs = torch.argmax(net_model(points), dim=1)\n",
        "      outputs = outputs.type(torch.float32)\n",
        "      loss = criterion(outputs, labels)\n",
        "      error_loss = errors_per_batch(outputs, labels)\n",
        "      train_one_loss_total.append(error_loss)\n",
        "      loss = loss.data.cpu().detach().numpy()\n",
        "      t_loss += loss\n",
        "      total_loss.append(t_loss)\n",
        "      loss_per_batch.append(loss)\n",
        "\n",
        "  train_loss = sum(train_one_loss_total)/len(train_set)\n",
        "  val_loss = sum(val_one_loss_total)/len(val_set)\n",
        "\n",
        "  return net_model, (train_loss, val_loss)"
      ],
      "metadata": {
        "id": "ml81VKkqYadN"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net_model,test_loader):\n",
        "  y_pred = pd.DataFrame(index=test_set_p.index, columns=['Survived'])\n",
        "  with torch.no_grad():\n",
        "    for i, (idx,points) in enumerate(test_loader):\n",
        "      if torch.cuda.is_available():\n",
        "        points = points.cuda()\n",
        "      outputs = torch.argmax(net_model(points), dim=1)\n",
        "      outputs = outputs.cpu().detach().numpy()\n",
        "      idx = idx.cpu().detach().numpy()\n",
        "      for j in range(idx.shape[0]):\n",
        "        y_pred.loc[idx[j]]['Survived'] = outputs[j]\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "V1_s8unKYoBq"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepering Train/Val/Test sets:"
      ],
      "metadata": {
        "id": "cf1WZkMOi0D2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Spliting to Train and Validation and Creaing Data Loaders*"
      ],
      "metadata": {
        "id": "ZaztxZ5PTJNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_p = pre_proccess(train_set_up)\n",
        "test_set_p = pre_proccess(test_set_up, isTrain=False)\n",
        "\n",
        "train_data, val_data = train_test_split(train_set_p,test_size=200, random_state=42)\n",
        "train_set = MyDataset(train_data)\n",
        "val_set = MyDataset(val_data)\n",
        "test_set = MyDataset(test_set_p, isTest=True)"
      ],
      "metadata": {
        "id": "dcnUuU_PY7eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine Tuning Hyper Parameters**"
      ],
      "metadata": {
        "id": "Hhr5k9ImTdw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lrs = [0.01,0.001,0.0001,0.00001]\n",
        "epochs = [10,15,20,25,30,50,100]\n",
        "b_size = [5,10,15,20,30,50]\n",
        "dropout_rate = [0.1,0.3,0.5,0.7]\n",
        "\n",
        "loss_arr = []\n",
        "for b in b_size:\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
        "                                            batch_size=b,\n",
        "                                            shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(dataset=val_set,\n",
        "                                            batch_size=b,\n",
        "                                            shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
        "                                            batch_size=b,\n",
        "                                             shuffle=True)\n",
        "  for l in lrs:\n",
        "    for e in epochs:\n",
        "      for d_out in dropout_rate:\n",
        "        learning_rate = l\n",
        "        num_epochs = e\n",
        "        batch_size = b\n",
        "        model, (t_loss,v_loss)= train(d_out,learning_rate, num_epochs,train_loader,val_loader)\n",
        "        if v_loss<0.3:\n",
        "          print(f'Learning Rate: {l}, epochs: {e}, batch size: {b},d_out: {d_out}, val loss: {v_loss}')\n",
        "        y_predict = predict(model, test_loader)\n",
        "        loss_arr.append((l,e,b,t_loss,v_loss))\n"
      ],
      "metadata": {
        "id": "wLB_7XErOZmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ea6a9c-4108-429a-beba-045db84dab9c"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate: 1e-05, epochs: 25, batch size: 10,d_out: 0.1, val loss: 0.25\n",
            "Learning Rate: 0.01, epochs: 10, batch size: 30,d_out: 0.5, val loss: 0.29\n",
            "Learning Rate: 0.01, epochs: 50, batch size: 30,d_out: 0.3, val loss: 0.275\n",
            "Learning Rate: 0.001, epochs: 100, batch size: 30,d_out: 0.1, val loss: 0.24\n",
            "Learning Rate: 0.01, epochs: 25, batch size: 50,d_out: 0.1, val loss: 0.285\n",
            "Learning Rate: 0.01, epochs: 30, batch size: 50,d_out: 0.1, val loss: 0.255\n",
            "Learning Rate: 1e-05, epochs: 10, batch size: 50,d_out: 0.5, val loss: 0.29\n",
            "Learning Rate: 1e-05, epochs: 50, batch size: 50,d_out: 0.1, val loss: 0.285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Finding the best hyper parameters combinations by sorting w.r.t validation loss*"
      ],
      "metadata": {
        "id": "xxQ-KO_HTnTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_arr.sort(key= lambda x: x[4])\n",
        "loss_arr[:10]"
      ],
      "metadata": {
        "id": "m4nuUrLwZC6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict:**"
      ],
      "metadata": {
        "id": "QIM8NDy1FbsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = 30\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
        "                                      batch_size=b,\n",
        "                                      shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_set,\n",
        "                                          batch_size=b,\n",
        "                                          shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
        "                                            batch_size=b,\n",
        "                                            shuffle=True)\n",
        "\n",
        "i=1\n",
        "while i>0:\n",
        "  d_out = 0.7\n",
        "  learning_rate = 0.001\n",
        "  num_epochs = 15\n",
        "  model, (t_loss,v_loss)= train(d_out,learning_rate, num_epochs,train_loader,val_loader)\n",
        "  print(f'loss: {v_loss}')\n",
        "  if v_loss<0.2:\n",
        "    break\n",
        "y_predict = predict(model, test_loader)\n",
        "y_predict.to_csv('prediction_simple_NN.csv')\n",
        "print(t_loss,v_loss)"
      ],
      "metadata": {
        "id": "cFoJuMevPKWY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}